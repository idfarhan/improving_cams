{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63152df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from src.era5_processing import era_read_file\n",
    "from src.cams_processing import cams_read_file, cams_0p75_read_file\n",
    "from src.odiac_processing import odiac_read_file\n",
    "from src.modis_processing import modis_ndvi_read_file\n",
    "from src.landscan_processing import landscan_read_file\n",
    "from src.gfed_processing import gfed_read_file\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d53c0b-cc4f-4d54-aa77-42d9afa4d3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directories\n",
    "\n",
    "cams_file = '/Volumes/My Passport/datasets/cams/xco2_0p75/cams_xco2_0p75.nc'\n",
    "cams_dir = '/Volumes/My Passport/datasets/cams/latest'\n",
    "era_dir = '/Volumes/My Passport/datasets/era_new/predicting_data'\n",
    "odiac_dir = '/Volumes/My Passport/datasets/odiac/1km'\n",
    "ndvi_dir = '/Volumes/My Passport/datasets/modis/MYD13C1v061'\n",
    "landscan_dir = '/Volumes/My Passport/datasets/landscan'\n",
    "gfed_dir = '/Volumes/My Passport/datasets/gfed/4.1'\n",
    "\n",
    "output_dir = 'predictions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c5bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doy(year):\n",
    "    start_date = datetime.date(year, 1, 1)\n",
    "    end_date = datetime.date(year, 12, 31)\n",
    "\n",
    "    current_date = start_date\n",
    "    date_list = []\n",
    "    while current_date <= end_date:\n",
    "        date_list.append(current_date.strftime(\"%Y-%m-%d\"))\n",
    "        current_date += datetime.timedelta(days=1)\n",
    "\n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca20f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import model\n",
    "model = load_model('xco2_dnn_model.h5')\n",
    "\n",
    "new_lats = np.arange(-89.625, 90, 0.75)\n",
    "new_lons = np.arange(-179.625, 180, 0.75)\n",
    "\n",
    "days = doy(2006) # Set the year\n",
    "for d in days:\n",
    "\n",
    "    filename = f'predicted_xco2_{d}.nc'\n",
    "    output_filename = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Check if the output file already exists\n",
    "    if os.path.exists(output_filename):\n",
    "        print(f\"File already exists for {d}, skipping...\")\n",
    "        continue\n",
    "        \n",
    "    print('Predicting data for', d)\n",
    "    odiac_ds = odiac_read_file(odiac_dir, d).rename(\n",
    "        {'x': 'longitude', 'y': 'latitude'}).drop_vars(['band', 'spatial_ref'])\n",
    "    odiac_interp = odiac_ds.interp(latitude=new_lats, longitude=new_lons,\n",
    "                                   kwargs={\"fill_value\": \"extrapolate\"})\n",
    "    ndvi_ds = modis_ndvi_read_file(ndvi_dir, d)\n",
    "    ndvi_interp = ndvi_ds.interp(latitude=new_lats, longitude=new_lons,\n",
    "                                 kwargs={\"fill_value\": \"extrapolate\"})\n",
    "    ndvi_interp['ndvi'] = xr.where((ndvi_interp['ndvi'] < 0) | ndvi_interp['ndvi'].isnull(),\n",
    "                                   -3000, ndvi_interp['ndvi'])\n",
    "    landscan_ds = landscan_read_file(landscan_dir, d).rename(\n",
    "        {'x': 'longitude', 'y': 'latitude'}).drop_vars(['band', 'spatial_ref'])\n",
    "    landscan_interp = landscan_ds.interp(latitude=new_lats, longitude=new_lons,\n",
    "                                         kwargs={\"fill_value\": \"extrapolate\"})\n",
    "    landscan_interp['landscan'] = xr.where((landscan_interp['landscan'] < 0) | landscan_interp['landscan'].isnull(),\n",
    "                                           0, landscan_interp['landscan'])\n",
    "    gfed_ds = gfed_read_file(gfed_dir, d)\n",
    "    gfed_interp = gfed_ds.interp(latitude=new_lats, longitude=new_lons,\n",
    "                                 kwargs={\"fill_value\": \"extrapolate\"})\n",
    "    \n",
    "    predicted_df = pd.DataFrame()\n",
    "    cams_tmp = cams_0p75_read_file(cams_file, d)\n",
    "    for cams_time in cams_tmp.time.values:\n",
    "        era_ds = era_read_file(era_dir, d).sel(time=cams_time).drop_vars('time')\n",
    "        era_interp = era_ds.interp(latitude=new_lats, longitude=new_lons,\n",
    "                                   kwargs={\"fill_value\": \"extrapolate\"})\n",
    "        cams_ds = (cams_tmp).sel(time=cams_time).drop_vars('time')\n",
    "        cams_interp = cams_ds.interp(latitude=new_lats, longitude=new_lons,\n",
    "                                     kwargs={\"fill_value\": \"extrapolate\"})\n",
    "        cams2_ds = cams_read_file(cams_dir, d).sel(time=cams_time).drop_vars('time')\n",
    "        cams2_ds_interp = cams2_ds.interp(latitude=new_lats, longitude=new_lons,\n",
    "                                          kwargs={\"fill_value\": \"extrapolate\"})\n",
    "        #cams2_ds_interp.drop_duplicates(inplace=True)\n",
    "        cams2_ds_interp['cams2'] = cams2_ds_interp['cams2'] * 1000000\n",
    "\n",
    "        predicting_ds = xr.merge([era_interp, cams_interp, cams2_ds_interp, odiac_interp, ndvi_interp, landscan_interp, gfed_interp])\n",
    "        predicting_df = predicting_ds.to_dataframe().reset_index()\n",
    "        input_df_tmp = predicting_df[['u10', 'v10', 'cams', 'cams2', 'odiac', 'ndvi', 'landscan', 'gfed']]\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        input_df = scaler.fit_transform(input_df_tmp)\n",
    "        \n",
    "        print('Predicting', cams_time)\n",
    "        prediction = model.predict(input_df).flatten()\n",
    "        predicted_df_tmp = pd.DataFrame({\n",
    "            'longitude': predicting_df['longitude'],\n",
    "            'latitude': predicting_df['latitude'],\n",
    "            'time': cams_time,\n",
    "            'XCO2': prediction})\n",
    "        \n",
    "        predicted_df = pd.concat([predicted_df, predicted_df_tmp], ignore_index=True)\n",
    "    \n",
    "    predicted_ds = xr.Dataset.from_dataframe(predicted_df.set_index(['time', 'latitude', 'longitude']))\n",
    "    predicted_ds['XCO2'].attrs['units'] = 'ppm'\n",
    "    predicted_ds['XCO2'].attrs['long_name'] = 'Column-averaged dry-air mole fraction of CO2'\n",
    "    predicted_ds['time'].attrs['long_name'] = 'Time'\n",
    "    predicted_ds['longitude'].attrs['units'] = 'degrees_east'\n",
    "    predicted_ds['longitude'].attrs['long_name'] = 'Longitude'\n",
    "    predicted_ds['latitude'].attrs['units'] = 'degrees_north'\n",
    "    predicted_ds['latitude'].attrs['long_name'] = 'Latitude'\n",
    "\n",
    "    output_filename = os.path.join(output_dir, filename)\n",
    "    predicted_ds.to_netcdf(output_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
